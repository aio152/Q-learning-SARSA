{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b88ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.075 0.775 0.075 0.075]\n",
      " [0.075 0.075 0.075 0.775]\n",
      " [0.775 0.075 0.075 0.075]\n",
      " [0.075 0.075 0.075 0.775]\n",
      " [0.775 0.075 0.075 0.075]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.075 0.075 0.775 0.075]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.075 0.075 0.075 0.775]\n",
      " [0.075 0.775 0.075 0.075]\n",
      " [0.775 0.075 0.075 0.075]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.075 0.075 0.775 0.075]\n",
      " [0.075 0.775 0.075 0.075]\n",
      " [0.25  0.25  0.25  0.25 ]]\n",
      "0.5171\n",
      "0.0155\n"
     ]
    }
   ],
   "source": [
    "#random walk in Frozen Lake, plus detection of end of episode\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1')#,map_name = \"4x4\",render_mode = \"human\")\n",
    "\n",
    "epsilon=0.3\n",
    "gamma=0.9\n",
    "alpha=0.01\n",
    "\n",
    "Q = numpy.zeros([env.observation_space.n, env.action_space.n])\n",
    "sommes = numpy.zeros([env.observation_space.n, env.action_space.n])\n",
    "compteurs = numpy.zeros([env.observation_space.n, env.action_space.n])\n",
    "#politique = numpy.ones([env.observation_space.n, env.action_space.n])\n",
    "#politique = politique*0.25\n",
    "politique = np.array([[0.025, 0.025, 0.925, 0.025],[0.025, 0.025, 0.025, 0.925],[0.925, 0.025, 0.025, 0.025], [0.025, 0.025, 0.025, 0.925],[0.925, 0.025, 0.025, 0.025],[0.25,  0.25,  0.25,  0.25 ],[0.025, 0.025, 0.925, 0.025],[0.25,  0.25,  0.25,  0.25 ],[0.025, 0.025, 0.025, 0.925],[0.025, 0.925, 0.025, 0.025],[0.925, 0.025, 0.025, 0.025],[0.25,  0.25,  0.25,  0.25 ],[0.25,  0.25,  0.25,  0.25 ],[0.025, 0.025, 0.925, 0.025],[0.025, 0.925, 0.025, 0.025],[0.25,  0.25,  0.25,  0.25 ]])\n",
    "\n",
    "nbEpisodes=50000\n",
    "\n",
    "for i in range(0, nbEpisodes):\n",
    "    etats=[]\n",
    "    actions=[]\n",
    "    recompenses=[]\n",
    "    state,prb=env.reset()\n",
    "    \n",
    "    firstState=state\n",
    "    endOfEpisode = False\n",
    "    nbSteps=0\n",
    "\n",
    "    while not endOfEpisode:\n",
    "        etats.append(state)\n",
    "\n",
    "        u=random.uniform(0,1)\n",
    "        somme=0\n",
    "        for action in range(0,env.action_space.n):\n",
    "            somme=somme+politique[state,action]\n",
    "            if u<somme:\n",
    "                break\n",
    "\n",
    "        actions.append(action)\n",
    "        next_state, reward, endOfEpisode, info ,prb= env.step(action) \n",
    "        recompenses.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        nbSteps=nbSteps+1\n",
    "\n",
    "    #une fois la simulation terminÃ©e on met Ã  jour nos valeurs\n",
    "    G=0\n",
    "    while actions:\n",
    "        etat=etats.pop()\n",
    "        action=actions.pop()\n",
    "        recompense=recompenses.pop()\n",
    "\n",
    "        G=G*gamma+recompense\n",
    "\n",
    "        if etat not in etats:\n",
    "            sommes[etat,action]=sommes[etat,action]+G\n",
    "            compteurs[etat,action]=compteurs[etat,action]+1\n",
    "            Q[etat,action]=sommes[etat,action]/compteurs[etat,action]\n",
    "\n",
    "            for action in range(0,4):\n",
    "                politique[etat,action]=epsilon/env.action_space.n\n",
    "\n",
    "            meilleureAction=numpy.argmax(Q[etat])\n",
    "            politique[etat,meilleureAction]=1-epsilon+epsilon/env.action_space.n\n",
    "print(politique)\n",
    "nbEpisodes=10000\n",
    "\n",
    "\n",
    "#env = gym.make('FrozenLake-v1',map_name = \"4x4\",render_mode = \"human\")\n",
    "\n",
    "#evaluation\n",
    "averageNumberSuccesses=0\n",
    "for i in range(0, nbEpisodes):\n",
    "    state,prb=env.reset()\n",
    "    \n",
    "    endOfEpisode = False\n",
    "    while not endOfEpisode:\n",
    "        action = numpy.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, endOfEpisode, info ,prb= env.step(action) \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if reward==1:\n",
    "        averageNumberSuccesses=averageNumberSuccesses+1\n",
    "averageNumberSuccesses=averageNumberSuccesses/nbEpisodes\n",
    "\n",
    "print(averageNumberSuccesses)\n",
    "\n",
    "\n",
    "\n",
    "#evaluation random strategy\n",
    "averageNumberSuccesses=0\n",
    "for i in range(0, nbEpisodes):\n",
    "    state,prb=env.reset()\n",
    "    \n",
    "    endOfEpisode = False\n",
    "    while not endOfEpisode:\n",
    "        next_state, reward, endOfEpisode, info,prb = env.step(env.action_space.sample()) \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if reward==1:\n",
    "        averageNumberSuccesses=averageNumberSuccesses+1\n",
    "averageNumberSuccesses=averageNumberSuccesses/nbEpisodes\n",
    "\n",
    "print(averageNumberSuccesses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966e009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff87b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
